Natural language processing and text reuse are well known problems, shared across humanities disciplines but also with computer scientists and engineers.

We used the following process. We gathered a corpus of codes of Civil Procedure and session laws from Google Books and the Hathi Trust, and identified just those pages of the documents that were relevant. We then used optical character recognition to convert those PDFs to plain text. We did some post processing on the texts to correct many obvious OCR errors and to fix problems such as hyphenated words. Natural language process in some fields has relied on perfectly clean, hand-coded and checked documents. Other projects, such as Viral Texts, have successfully used even very bad OCR from the Chronicling America dataset. In a few cases we hand transcribed codes which had no print version. Our project stood in the middle: we had a tolerably clean corpus of texts, yet used methods that were robust to noisy OCR.

Our corpus comprised 113 documents of about 8 million words. This is decidedly in the realm of middle data. Much discussion of text mining has focused on so-called big data, which may mean many things, but in its technical meaning means data which is too large to fit within the memory of a single computer. Big data approaches have often focused on solving large problems such as changing use of language, literary periodization, and so on. Without denying the validity of these big data projects, we think that the most promising route for historians is in the realm of middle data. We note that our dataset has several characteristics of middle data. First, the amount of data is sufficiently large that a historian could not simply read the texts. We note that the size of the corpus, 8 million words, but also the quality of the corpus: the point of reading the corpus was not to close read for meaning, but find patterns which close reading could not reveal. Second, the data did not require any specialized computing power: a laptop would be adequate for our methods. Yet the data (as will be explained below) was sufficiently large that a naive approach to computation, even on the most powerful hardware, could never compute the matches. This required borrowings of algorithms developed by computer scientists and, in one case, geneticists, to compute the matches in a reasonable time. Third, the corpus was curated and carefully circumscribed. We used every document that fits within our description of codes of civil procedure. We note that there are many examples of corpora---especially for periods outside of onerous copyright restrictions---that can be gathered. Within legal history, other kinds of codes and statutes, as well as legal treatises. Stephen Robertson, for instance, has used legal treatises to study the law of rape through close reading, and this method could be used to study treatises treatments of X, Y, and Z. But outside the realm of legal history, there are many corpora. One example is publications of the American Tract Society, which frequently borrow from one another. Our particular method is suitable for documents which borrow heavily, but more broadly middle data focuses on well defined corpora which can be reasonably complete. And most important, unlike big data, these middle data corpora are amenable to a hybrid method, using the tools of digital research yet also amenable to traditional historical methods, thus allowing both methods to inform one another.

Having created versions of the {TODO} documents in our corpus, we then split them into sections. The Field Codes, like most legal documents, are organized in article's, titles, and sections. We are thus able to use those keywords and other symbols (e.g., §) to split the legal codes into separate documents. We prioritized cleaning up the section headers to make it possible to split the documents accurately. The New York 1850 code thus became 2,126 {TODO: Update} separate documents. We tested the splitting of the codes into sections for accuracy, in several ways. We counted the words in each newly split document to make sure that we were not lumping many sections together. And we compared the number of split documents to the total number of sections in the codes. For instance, the New York 1850 code had 1,885 numbered sections, and we split it into 2,126 sections. The difference is explained since our splitting methods also put many tables of contents and other such intermediary sections into their own documents, which also has the advantage of cleaning out such extraneous text. The key here is that we took advantage of the characteristics of the documents *and* our theory about how the documents borrowed from one another. As described above, we know from evidence external to the codes that they borrowed the codes section by section. Our model of code borrowing was thus informed by other historical evidence.

Having thus sectioned the texts, we needed a method for detecting borrowings that is robust to both accidental variation because of OCR mistakes and to the intentional editing and revisions undertaken by different code commissions. To illustrate the problem, consider these two sections of the codes on serving summonses. From New York 1850:

> § 628. The summons must be served by delivering a copy thereof, as follows:
>
> 1.  If the suit be against a corporation, to the president or other head of the corporation, secretary, cashier, or managing agent thereof:
>
> 2.  If against a minor under the age of fourteen years, to such minor personally, and also to his father, mother, ... [^1]
>
> And from California 1851:

> § 29. The summons shall be served by delivering a copy thereof attached to the certified copy of the complaint</span> as follows:
>
> 1st. If the suit be against a corporation, to the president or other head of the corporation, secretary, cashier, or managing agent thereof:
>
> 2d. If against a minor under the age of fourteen years, to such minor personally, and also to his father, mother, or guardian; or if there ... [^2]

The borrowing is obvious, but variations make it complicated to detect the similarity algorithmically. In the example above, California adds the words "attached to the certified copy of the complaint." Sometimes the variations are due to local circumstances, as in the example below, where the California code inserts the local names of its courts and sensibly requires that actions happen in San Francisco instead of New York. In addition to actual changes in the text, imperfect OCR adds noise. Though we have taken some pains to get good OCR and were mostly successful, I think, the noise still adds an additional challenge.

The common practice is to split these documents into n-grams: that is, a sequence of `n` words that overlap with one another. In practice, we found `n = 5` to provide acceptable results. These n-grams can be compared together. They avoid incidental matches for common constructions or stock legal phrases, while also providing matches despite the accidental and deliberate variations.[^3]

  NY 1850                        CA 1851                              match?
  ------------------------------ ------------------------------------ --------
  the summons must be served     the summons shall be served          no
  summons must be served by      summons shall be serve by            no
  must be served by delivering   shall be served by delivering        no
  be served by delivering a      be served by delivering a            yes
  served by delivering a copy    served by delivering a copy          yes
  by delivering a copy thereof   by delivering a copy thereof         yes
  delivering a copy thereof as   delivering a copy thereof attached   no
  a copy thereof as follows      a copy thereof attached to           no
  ...                            ...                                  
  as follows if the suit         as follows if the suit               yes
  follows if the suit be         follows if the suit be               yes

Having created the n-gram tokens for each document, we could treat each document as a mathematical set of tokens. A widely used measure of similarity is Jaccard similarity, which divides the number of token shared by the documents by the total number of tokens in both documents. This ratio can range between `0` and `1`. For the two documents above, the Jaccard similarity is `0.506`, high enough to indicate that there is a strong probability that the two documents borrow from one another, but low enough to indicate that there are significant variations between the documents.[^4]

We then calculated the similarities between all the documents. The difficulty is that the number of comparisons to be made grows geometrically with the number of documents.^[The exact number of comparisons to be made is given by ((d^2)- d) / 2), where d is the number of documents.] So even our middle sized data would require on the order of 3.2 billion comparisons between documents. While something like ten thousand comparisons per second can be made for documents of the length that we are working with, that still would require approximately four days worth of computation. Even our middle sized data therefore required a non-naive approach. 

We implemented therefore implemented a version of the minhash/locality-sensitive hashing algorithm. While the specific details of this algorithm are beyond the scope of this article, the basic principle can be explained. This algorithm generates a signature for each document. All of signatures can be compared to one another to extract pairs of documents that are likely to be matches. The important point is that the signatures approximate the Jaccard similarity of the documents, so that by setting the parameters of the algorithm, one can set the sensitivity

^[A.Z. Broder, “On the Resemblance and Containment of Documents,” in *Compression and Complexity of Sequences 1997. Proceedings*, 1997, 21–29, doi:10.1109/SEQUEN.1997.666900; Jure Leskovec, Anand Rajaraman, and Jeff Ullman, Mining of Massive Datasets (Cambridge University Press, 2011).]

We can sum up the contributions as follows. We have made no new algorithms---we are after all historians, not computer scientists. But we have adapted 

[^1]: *The Code of Civil Procedure of the State of New-York* (1850), title 4, section 628, p. [257](http://books.google.com/books?id=9HEbAQAAIAAJ&dq=%22Documents%20of%20the%20Assembly%20of%20the%20State%20of%20New%20York%22%201850%20david%20graham&pg=PA257#v=onepage&q&f=false).

[^2]: "An Act to regulate proceedings in Civil Cases, in the Courts of Justice of this State," title 3, section 29, in *The Statutes of California, Passed at the Second Session of the Legislature* (1851), [55](http://books.google.com/books?id=4PHEAAAAIAAJ&ots=TDfCSa1x9C&dq=laws%20of%20california&pg=PA51#v=onepage&q&f=false).

[^3]: We actually took an additional step and represented these tokens as hashes, that is, we turned each n-gram into a single integer representation. This is a common practice.

[^4]: The formal definition of Jaccard similarity is the ratio between the intersection of the sets and the union of the sets. Set similarity counts each token only once for each time it appears in a set; while bag similarity counts duplicated token more than once. In practice the difference between bag and set similarity had no effect on our analysis, since `n = 5` meant that there were very few duplicated tokens in each document. For example, in the two sections used as an example above, there were no duplicated tokens within each document.
