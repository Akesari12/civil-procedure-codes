Natural language processing and text reuse are well known problems, shared across humanities disciplines but also with computer scientists and engineers.

The following process was used. We gathered a corpus of codes of Civil Procedure and session laws from Google Books and the Hathi Trust, and identified just those pages of the documents that were relevant. We then used optical character recognition to convert those PDFs to plain text. We did some post processing on the texts to correct many obvious OCR errors and to fix problems such as hyphenated words. Natural language process in some fields has relied on perfectly clean, hand-coded and checked documents. Other projects, such as Viral Texts, have successfully used even very bad OCR from the Chronicling America dataset. In a few cases we hand transcribed codes which had no print version. Our project stood in the middle: we had a tolerably clean corpus of texts, yet used methods that were robust to noisy OCR. 

Our corpus comprised 113 documents of about 8 million words. This is decidedly in the realm of middle data. Much discussion of text mining has focused on so-called big data, which may mean many things, but in its technical meaning means data which is too large to fit within the memory of a single computer. Big data approaches have often focused on solving large problems such as changing use of language, literary periodization, and so on.  Without denying the validity of these big data projects, we think that the most promising route for historians is in the realm of middle data. We note that our dataset has several characteristics of middle data. First, the amount of data is sufficiently large that a historian could not simply read the texts. We note that the size of the corpus, 8 million words, but also the quality of the corpus: the point of reading the corpus was not to close read for meaning, but find patterns which close reading could not reveal. Second, the data did not require any specialized computing power: a laptop would be adequate for our methods. Yet the data (as will be explained below) was sufficiently large that a naive approach to computation, even on the most powerful hardware, could never compute the matches. This required borrowings of algorithms developed by computer scientists and, in one case, geneticists, to compute the matches in a reasonable time. Third, the corpus was curated and carefully circumscribed. We used every document that fits within our description of codes of civil procedure. We note that there are many examples of corpora---especially for periods outside of onerous copyright restrictions---that can be gathered. Within legal history, other kinds of codes and statutes, as well as legal treatises. Stephen Robertson, for instance, has used legal treatises to study the law of rape through close reading, and this method could be used to study treatises treatments of X, Y, and Z. But outside the realm of legal history, there are many corpora. One example is publications of the American Tract Society, which frequently borrow from one another. Our particular method is suitable for documents which borrow heavily, but more broadly middle data focuses on well defined corpora which can be reasonably complete. And most important, unlike big data, these middle data corpora are amenable to a hybrid method, using the tools of digital research yet also amenable to traditional historical methods, thus allowing both methods to inform one another.

Having created versions of the {TODO} documents in our corpus, 
