---
title: "The Migration of the Field Code"
author: "Kellen Funk and Lincoln Mullen"
output: tufte::tufte_handout
date: "February 17, 2016"
---

\begin{abstract}
This handout describes our work in progress for the Digital Humanities Working Group at George Mason University.\\
E-mail: \url{lincoln@lincolnmullen.com}, \url{kfunk@princeton.edu}
\end{abstract}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = NA)
library(tufte)
library(textreuse)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(readr)
library(knitr)
library(igraph)
library(ggraph)
library(stringr)
source("../../R/helper.R")
source("../../R/section-matches.R")
source("../../R/spectrogram.R")
load("../../cache/corpus-lsh.rda")
load("../../cache/network-graphs.rda")
clusters <- read_csv("../../out/clusters.csv")
```

# The Field Code

\begin{marginfigure}
\includegraphics{../aha2016/field.jpg}
\caption{David Dudley Field II (1805--1894).}
\end{marginfigure}

After the American Revolution, most states were common law jurisdictions, sometimes with courts of chancery. These courts had a complex system of pleading defined by case law. By the 1840s, lawyers and the mercantile classes called for the simplification and rationalization of civil procedure through codification. Factions for and against codification debated about whether codes were laws passed by democratic legislatures or anti-democratic legal elites, whether codification served only the needs of wealthy capitalists, and whether the purity of Anglo-Saxon civilization derived from the common law would be maintained. The economic capital New York was the first state to codify its procedure in 1848, thanks to the efforts of David Dudly Field. By the end of the century, New York's Field Code became the model for the codes of civil procedure in most states.

\begin{figure}
\includegraphics{../aha2016/field-code-states-map.jpeg}
\caption{Field Code states by date of first enactment. Many states subsequently revised their codes of civil procedure.}
\end{figure}

Legal historians have long known that the Field code spread to other jurisdictions. Beyond the mere fact of its adoption, however, no one has studied the content of the borrowings. Which codes borrowed from each other? Which sections were borrowed, and howe were they modified? What were the patterns and structures of borrowings and of innovations? To answer these questions we gathered a corpus of 115 codes and statutes of civil procedure from 1806 to 1933 containing about 7.6 million words, then algorithmically detected the borrowings within that corpus.

The aim of this handout is to describe our contributions to digital methods in legal history (and other fields), and to outline the interpretations we have drawn about the migration of the Field Code. Our method is one of two common approaches in computational history. We created a dataset to answer a given set of questions; a different approach is to take the sources as given and explore the data to see what questions it raises.^[This approach features "middle data" (not "big data") which we might define as data that is too small for distributed computation but too big for naive algorithms. Alternatively, it is data where the size of the sample approaches the size of the population, but where the population is strictly constrained by the research problem.] Our historical findings describe how American legal practice became standardized around a New York code yet varied by region.

# How we found the borrowings

We found out how the codes borrowed from one another by splitting the codes into sections and comparing each section to every other section.^[The data and code to re-run our analyses are available in a GitHub repository: <https://github.com/lmullen/civil-procedure-codes>. We generalized our method in Lincoln Mullen, "textreuse: Detect Text Reuse and Document Similarity," R package version 0.1.2 (2015): <https://github.com/ropensci/textreuse>, which was peer-reviewed by rOpenSci.] This process in essence mimicked the way that nineteenth-century code commissioners literally cut and pasted sections from other jurisidictions. 

## Preparing the corpus

Having identified all of the relevant laws of civil procedure in the nineteenth-century, including codes, session laws, and statutes, we used OCR software to create plain-text versions of the codes. These OCR files received only a light cleaning: we edited the section markers by hand as necessary, and wrote a script to fix the most obvious OCR errors.

We then split each section of each code into its own text file. The corpus contains nearly 98,000 sections. Below is a sample file containing a single section from a single code.^[1851 California Laws 74, ยง151. This corresponds to our file `CA1851-001660.txt`. Notice the minor OCR errors.]

```
151. An issue arises when a fact or conclusion of law 
is maintained by the one party, and is controverted 
by theothsr. Issues are of two kinds: lst. Of law: 
and,  _ 2d. or fact. if?
```

## Tokenizing and measuring similarity

The next stage in our process was to measure the similarity of the sections one to another. We first had to tokenize the text, meaning that  After experimenting we found that five-grams worked well.^[We also hashed the tokens, meaning that we converted them to integer representations which permits considerable savings of memory.]  The use of n-grams, which is a ubiquitious technique, permits redudancy of phrasing, intentional word changes, and OCR errors. Below are the first five tokens from the section above.

```{r}
tokenize_ngrams("151. An issue arises when a fact or conclusion of law 
is maintained by the one party, and is controverted 
by theothsr. Issues are of two kinds: lst. Of law: 
and,  _ 2d. or fact. if?", n = 5) %>% head(5)
```

Next we used the Jaccard similarity score for measuring document similarity. That measure, treating the tokens as a set, is the ratio of shared tokens to the ratio of total tokens in two documents. The result is a single number ranging from 0 (complete dissimilarity) to 1 (complete similarity).^[From MMDS. Formally, the definition of the Jaccard similarity score is $$J(A, B) = \frac{ | A \cap B| }{ | A \cup B|}$$] For instance, the section from the California 1851 code above was derived from section 756 of the New York 1850 code (`NY1850-008350.txt`). Because of changes to the wording and OCR errors, the Jaccard similarity between the two sections was `r jaccard_similarity(sections[["NY1850-008350"]], sections[["CA1851-001660"]]) %>% round(3)`. By carefully checking matching sections versus scores, we arrived at a rule of thumb that a Jaccard similarity score greater than 0.15 likely indicated a match, and a score greater than 0.2 almost certainly indicated a match.

## Computing similarity for the entire corpus

```{r}
num_comparisons <- function(n) { ((n * (n - 1)) / 2) }
billionize <- function(x) { round(x / 1e9, 1)}
```

The next step was to compute similarity for every section in the corpus. The difficulty is that to compare every section to every other section would require an enormous number of comparisons: approximately `r length(sections) %>% num_comparisons() %>% billionize()` billion for our corpus.^[Assuming that the similarity measure is bi-directional, the number of pairwise comparisons in a corpus is given by $(n^2-n) / {2}$.] Most of these comparisons would be wasted, since most sections have zero relationship to most other sections.

```{r, fig.margin = TRUE, fig.cap = "This chart shows the threshold S-curves for various settings of the minhash/LSH algorithm. The x-axis shows the actual measured Jaccard similarity of the two documents; the y-axis shows the probability that they will be marked as a match. We used the settings for the leftmost curve, guaranteeing that we detected all matches above a similarity of 0.2."}
prob_for_graph1 <- Vectorize(function(x) lsh_probability(120, 60, x))
prob_for_graph3 <- Vectorize(function(x) lsh_probability(120, 30, x))
prob_for_graph5 <- Vectorize(function(x) lsh_probability(120, 10, x))
input <- data_frame(x = seq(0, 1, 0.001))
ggplot(input, aes(x = x)) + 
  stat_function(fun = prob_for_graph1, linetype = 1) +
  stat_function(fun = prob_for_graph3, linetype = 2) +
  stat_function(fun = prob_for_graph5, linetype = 2) +
  theme_tufte() +
  labs(x = "Jaccard similarity",
       y = "Probability",
       title = "Probability of a match for a given similarity") +
  scale_x_continuous(breaks = seq(0, 1, .2))
```

We implemented the minhash/locality sensitive hashing (LSH) algorithm to detect candidate pairs, i.e., documents which were likely to be matches. This algorithm works by extracting a set number of random tokens from each document, allowing the documents to be represented uniformly and compactly. Then those random tokens are grouped into subsets. If any two documents have a matching subset, then they are considered a candidate pair. This algorithm has several useful properties. It approximates the Jaccard similarity of the two documents. The algorithm requires a computation for each document, not each pair of documents, so the compute time grows linearly not geometrically. And by controlling various parameters, one can determine a threshold similarity score above which one is likely to find a match and unlikely to find an inaccurate match.

Once we had detected the candidate pairs, we measured the actual Jaccard similarity for those pairs. The result was a sparse matrix of similarity scores, with rows and columns for each section in the corpus. A tiny subset is shown below.^[Notice that all of the examples given from here on relate to the same set of codes from California, Oregon, and Washington, in order to demonstrate how the different parts of our method work together.]

```{r, fig.fullwidth = TRUE}
a_cluster <- clusters %>% filter(cluster_id == 676) %>% `$`("doc")
sample_matrix <- sections[a_cluster] %>% 
  pairwise_compare(jaccard_similarity, progress = FALSE, directional = TRUE) %>% 
  round(2) 
sample_matrix[is.na(sample_matrix)] <- ""
knitr::kable(sample_matrix, align = "c", 
             caption = "A subset of the similarity matrix")
```

This matrix, however, required further filtering based on what we knew about the process of borrowing from other methods. For instance, a code from 1851 could obviously not have borrowed from a code from 1877. Furthermore, in chains of borrowing (e.g., MT1895 $\rightarrow$ CA1868 $\rightarrow$ CA1851 $\rightarrow$ NY1850) the latest section might have a high similarity to all of its parents, but was in fact borrowed only from the most recent parent. We therefore filtered the similarity matrix to remove (1) matches within the same code; (2) anachronistic matches; (3) spurious matches beneath a certain threshold. Then if a section had multiple matches, we kept the match from (5) the chronologically closest code from the same state unless (6) there was a substantially higher match from a different code. The result was a sparse matrix of the most likely matches for each section.

# Learning from the borrowings

A similarity matrix is a common input to many other visualizations and algorithms. We used the matrix of best borrowings to learn how the codes were borrowed in several ways.

## Clustering the borrowings

We used a clustering algorithm to group similar sections together. There are innumerable clustering algorithms, but we needed one that could work with a sparse matrix and one whose assumptions aligned with the problem we were working on. We used the affinity propagation clustering algorithm.^[Brendan J. Frey and Delbert Dueck, "Clustering by Passing Messages Between Data Points," *Science* 315 (2007): 972--976.] That algorithm assumes that there is an "exemplar" item in each cluster. That assumption exactly matches the case with the Field code, where a single section (likely from NY1850) had many borrowings. Furthermore, even though the affinity propagation clustering algorithm did not fully converge on our peculiar dataset, it did do an adequate job of clustering the documents initially. Because there was an exemplar section for each cluster, we were able to merge clusters where the exemplar sections had a high Jaccard similarity score. 

The result was a set of approximately 2,900 clusters which contained at least five sections. This probably overstates the number of ur-sections in the corpus. Each cluster contained a list of the section IDs that belonged to it. The biggest cluster, for instance, contained 103 sections, which concerned the use of affidavits in pleading. Scholars in digital literary studies often speak of "deformance" of texts.^[Stephen Ramsay, *Reading Machines: Toward an Algorithmic Criticism* (University of Illinois Press, 2011), ch. 3.] Clustering the sections of the codes deformed them because it pulled the sections out of the codes from which they had been embedded in, and placed them in chronological order. This allowed us to see the development and spread of the law. You might consider this a method of distant reading which then permits a kind of close reading. Consider this (admittedly more important than usual) excerpt from one of the clusters which concerned the competence of witnesses:

```
Cluster ID: 10382          Documents in cluster: 20
Exemplar: OR1854-003380    Earliest: NY1850-018680

NY1850-018680 ------------------------------------------------------------------
1709. The following persons are not admissible: 1. Those who are of unsound mind
at the time of their production for examination: 2. Children under ten years of
age, who appear incapable of receiving just impressions of the facts, respecting
which they are examined, or of relating them truly.

CA1851-004380 ------------------------------------------------------------------
394. The following persons shall not be witnesses: โ lst. Those who areof
unsound mind at the time of their production for examination: 2d. Children under
ten years of age, who appear incapable of receiving just impressions of the
facts respecting which they are examined, or of relating them truly: and, 3d.
Indians, or persons having one fourth or more of Indian blood, in an action or
proceeding to which a white person is a party: 4th. Negroes, or persons having
one half or more Negro blood, in an action or proceeding to which a white person
is a party.

OR1854-003380 ------------------------------------------------------------------
6. The following persons shall not be competent to testify 1. Those who are of
unsound mind, or intoxicated at the time of their production for examination ;
2. Children under ten years of age, who appear incapable of receiving just
impressions of the facts respecting which they are examined, or of relating them
truly; 4. Negroes, mulattocs and Indians, or persons one half or more of Indian
blood, in an action or proceeding to which a white person is a party.
```

## Networks of borrowings

A matrix of similarities can be thought of as an adjacency matrix to a network graph. We created network graph of the code to code borrowings. In an effort not to make this a typical hairball, we filtered the edges so that each code was connected to another code only if it borrowed at least fifty sections.

```{r, fig.fullwidth=TRUE, fig.cap="Code to code borrowings.", fig.height=3}
set.seed(21000)
ggraph(codes_g, "igraph", algorithm = "nicely") +
  geom_edge_fan(aes(edge_width = sections_borrowed, 
                    alpha = sections_borrowed),
                arrow = arrow(type = "closed", ends = "first",
                              length = unit(0.10, "inches"),
                              angle = 15)) +
  geom_node_point(aes(color = as.factor(distance)), size = 2) +
  scale_edge_width("Sections borrowed", range = c(0.25, 1), guide = "none") + 
  scale_edge_alpha(range = c(0.3, 0.6), guide = "none") +
  scale_color_brewer(palette = "Set1", "Distance from a NY code") +
  ggforce::theme_no_axes(base_size = 4, base_family = "serif") +
  geom_node_text(aes(label = name), size = 1.5) +
  theme(legend.position = "bottom", 
        panel.border = element_blank())
```

This network shows the structure of the borrowings. In particular, it demonstrates the centrality of New York's code, especially the eponymous Field Code of 1850. The New York code then became the basis of other families of codes. Those codes heavily edited NY1850, but within the families (which were often based on the proximity of states) the borrowings tended to be much more close. Yet states could and did sometimes borrow from multiple states. Notice that this network graph moves both geographically an chronologically. It is later codes which tend to be on the outside of the network diagram, and they are the most distant from the original Field Code in both time and borrowings.^[If we move up a level of abstraction we can consider not code to code but state to state borrowings. This shows us the pattern of borrowings more clearly, albeit at a considerable loss of detail. While that is a constant challenge in any kind of visualization work, it is especially a challenge here where we know that our measures of borrowings can be inaccurate for individual sections, though they are reliable on the whole.]


```{r, eval=FALSE, fig.fullwidth=TRUE, fig.cap="State to state borrowings.", fig.height=3}
set.seed(210)
ggraph(states_g, "igraph", algorithm = "graphopt") +
  geom_edge_fan(aes(edge_width = sections_borrowed, 
                    alpha = sections_borrowed),
                arrow = arrow(type = "closed", ends = "first",
                              length = unit(0.10, "inches"),
                              angle = 15)) +
  geom_node_point(size = 3, aes(color = region)) +
  scale_edge_width("Sections borrowed", range = c(0.25, 1), guide = "none") + 
  scale_edge_alpha(range = c(0.3, 0.6), guide = "none") +
  scale_color_brewer(palette = "Dark2", "Region") +
  ggforce::theme_no_axes(base_size = 4, base_family = "serif") +
  geom_node_text(aes(label = name), size = 1.5) +
  theme(legend.position = "bottom", 
        panel.border = element_blank())
```

## Borrowings within each code

Finally, each of those nodes on the network diagram can be individualy investigated to get a more detailed perspective on how the codes were borrowed. We did this by creating a kind of "spectrograph" or fingerprint of which sections in a code were borrowed from which other codes. Take the case of Washington state's 1855 code. 


```{r, fig.cap="Borrowed sections in Washington's 1855 code. Each section in the code is represented by a square, the color of which indicates where it was borrowed from."}
spectrogram("WA1855", best_matches, white_list = 4)
```

We can observe that most of the code is borrowed from some source or another. While there is a fair bit of "other" which are probably sections which we cannot adequately attribute, the pattern of borrowing is clear: IN1852 and OR1854 provide the majority of the borrowings, and they do so in two bands. This pattern is curious, because one might expect Washington to borrow from Oregon and California but Indiana is not an obvious choice. And why do the borrowings fall into these bands? Regulations on judgment were borrowed from Oregon, while enforcement provisions came from Indiana. We think this is because one of the Washington code commissioners, Edward Lander, was an Indiana appelate judge from 1850 to 1853, while another commissioner, William Strong, was a justice of the Oregon Supreme Court in the same years. In other words, the law in this case moved with judges and lawyers who picked the parts they knew best.

Of course we can also demonstrate in an obvious way when codes were borrwed heavily from one source, yet we can also note the unusual places in the code where apparently later innovations from Oregon were borrowed, or where the California codes were not strictly borrowed.

```{r}
spectrogram("NV1861", best_matches, white_list = 4)

spectrogram("NM1865", best_matches, white_list = 8)
```

# Conclusions

There are two sets of conclusions, or at least observations, which can be drawn from this work in progress. 

We think that the methodology of this kind of work has a number of applications to historical problems. First, question framing. One very useful thing to do when framing a digital historical problem is to begin with exploratory data analysis of a large corpus---to let the sources drive the questions. We have modeled an approach that is at the opposite side of the interpretative spiral, by forming a corpus to answer an already formed historical question. The use of *medium data* permits us a close relationship between traditional and digital historical methods, which is our second point. Our methods produced useful historical knowledge because the digital methods were closely connected to assumptions we knew to be true about the data from our traditional historical work. To give a brief list of those harmonies: we knew that code commissioners worked with "the scissors and paste-pot," as political debates about the codes frequently complained, and we knew from examining codes in thea archives that commisions literally marked up the legislation of other states.^[*Rocky Mountain News*, January 20, 1877.] That knowledge, particularly the knowledge that the borrowings happened at the level of the section, justified our use of the minhash/LSH algorithm. Our knowledge that particular sections were frequently borrowed and thus were exemplars justified our use of the affinity propagration algorithm. And our assumptions about how codes were passed from state to state made network analysis an obvious fit.^[We might call these an example of the "no free lunch theorem" in action. See David Robinson, "K-means clustering is not a free lunch," *Variance Explained*, January 16, 2015: <http://varianceexplained.org/r/kmeans-free-lunch/>.] And third, we think that this method is widely applicable to different historical questions and sources, almost without modification in the case of corpora where the documents can be readily divided into sections. This method could be pursued in other legal documents such as treatises or statutes; it could also be pursued in sources of other domains, such as hymnbooks or tracts.

Finally, the article that we are writing about the Field code will bring together what we have learned through traditional historical work and the digital methods described above. But we can sum up what we have learned primarily from the digital methods. This study is significant because it gets at the heart of lawmaking in U.S. history. Lawyers and judges, politicians and newspaper editors warred over whether codes that were drafted by commissioners and borrowed wholesale from one another were actually democratic laws or the imposition of a legal elite. Furthermore, these borrowings called into question the extent to which the United States was actually a federal system of laws. We have documented the scope and content of the exact borrowings. We have shown the dominance of New York's laws in a supposedly federal system, and shown exactly the extent to which code commissioners borrowed from that code and its descendants. Economic centers, which needed certain kinds of laws tied up with the expansion of markets and American capitalism, were in fact legal centers. But we have also demonstrated that regional distinctions were also significant, so that regional economic centers like California and Ohio generated separate Western and Midwestern families of codes. Finally we have shown how haphazard the process of codification could be, as in the case of the Washington code cited above where judges from jurisdictions borrowed laws that they were familiar with, and in the case of New Mexico, which borrowed Missouri's laws because those were the law books which were actually available.

The specific provisions of the Field code aren't in continued use in every jurisdiction, (though in states like Missouri the Field Code provisions survive virtually unchanged) but the basic ideas embedded in the Field Code are standard practice which have spread throughout the entire country. Without too much exaggeration we might say that our method has revealed the spine of modern American legal practice.
